This MPI code implements a parallelized matrix operation by assigning each process to handle a dedicated sub-matrix of the larger matrices `D` and `D0`, sized at (n/sqrt(P)) x (n/sqrt(P)), where P is the number of processes, structured as the square of an integer. This design allows for each process to allocate memory specifically for its sub-matrix, making memory usage efficient and limiting data load. Additionally, the program allocates up to four extra 1-D arrays of size (n/sqrt(P)) to support intermediate calculations. This allocation method ensures that memory usage remains manageable, particularly in large-scale matrices, while each process is able to perform local calculations within its sub-matrix.

To facilitate parallelism and optimize inter-process communication, the code establishes even and odd process groups, organizing processes into communicators for efficient data handling. Using `MPI_Group_incl` to create two groups based on process ranks (even and odd) allows each communicator (`even_comm` and `odd_comm`) to manage subsets of processes that handle specific rows in the matrix. Within each communicator, `MPI_Bcast` is used to share row data among group members in a way that avoids unnecessary data transfer to processes outside the group. Additionally, `MPI_Allreduce` ensures that minimum values are synchronized across processes, allowing each process to have access to the global minimum distance values required for the Floyd-Warshall algorithm.

The code structure is designed to maintain a communication complexity of O(n/sqrt(P) * log_2(sqrt(P))) per iteration, limiting communication to essential exchanges within each sub-matrix. By reducing inter-process communication through the communicator setup and carefully controlling data flow, the algorithm achieves efficient scaling for larger matrix sizes, as each process only shares data within its communicator group. The code's efficient memory handling and well-organized data sharing enable it to handle large matrices effectively, fulfilling requirements for optimized resource usage in high-performance computing environments.
